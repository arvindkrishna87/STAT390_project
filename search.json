[
  {
    "objectID": "step3.html",
    "href": "step3.html",
    "title": "Step 3: Annotate Slices",
    "section": "",
    "text": "Step 3: annotating is happeneing in parallel with Step 4: patching\nAim: identify different classes in the epithelium so we can train a model with ground truth\nWe have sent matched slices to the stakeholder for them to annotate with Benign, Low grade, and High grade C-MIL. This will provide a ground truth for our future model.\n\n\n\n\n\nIn the meantime, we are working on our patching algorithm."
  },
  {
    "objectID": "litreview.html",
    "href": "litreview.html",
    "title": "Literature Review",
    "section": "",
    "text": "Adaptive Pooling: (Used in Step 4)\n\n\n\n\n\n\nLarge Batch and Patch Size Training for Medical Image Segmentation:\nArticle Link\n\nThis article highlights the performance improvements gained by adjusting patch and batch sizes, suggesting that larger patches improve segmentation accuracy up to a certain limit, beyond which performance plateaus or even decreases.\nAdaptive pooling could be used to create variable patch sizes within a single image, enabling finer details in complex areas and larger patches for broader context in simpler areas.\nBy using adaptive pooling, you could manage patch sizes more dynamically, potentially overcoming the plateau effect observed with fixed large patch sizes.\nThis would ensure that each region of the image is analyzed at an optimal scale, leading to potentially higher accuracy without the drawbacks of uniform large patches.\n\n\n\n\n\n\n\n\n\n\nCross View Transformer: (Used in Step 4)\n\n\n\n\n\n\nPurpose: Cross-view transformers are ideal for aligning images from different modalities (or stains) without exact pixel-level registration. They enable spatial feature alignment through learned transformations, which is helpful when the images are stained differently and thus might not naturally align perfectly.\nBenefit: Using this alignment allows our algorithm to recognize similar regions across stains, orienting them correctly and enabling the same-region patches to be matched across the stains. The algorithm would learn to map similar features across images without requiring identical sizes, which suits our no-resize constraint.\nHow to Implement:\n\nInput the Three Stains: For each patient’s tissue sample, create multi-resolution representations for each of the three stains (sox10, melana, and H&E).\nAlign Using Cross-View Transformers: Apply cross-view alignment to identify similar regions across stains by mapping corresponding features. This step ensures that the patches you sample come from the same tissue regions across the three stains.\nExtract Aligned Patches: Use the aligned regions to sample patches that are similarly oriented and positioned, ensuring consistency.\nApply Controlled Padding (if Needed): Pad the patches to make them uniform in size, allowing them to be used in the classification model.\n\nSummary: Ultimately, the multi-resolution network will capture different levels of detail from each stain, ensuring that both fine cellular structures and broader tissue architectures are represented. This helps to make the model robust to slight size or scale variations in patches. The cross-view alignment will allow our algorithm to “match” regions across different stains based on learned features rather than exact spatial coordinates. This is especially helpful when we have patches of different stains that aren’t perfectly aligned but should correspond to the same tissue region.\n\n\n\n\n\n\n\n\n\nAdvantages of square patches: (Used in step 4)\n\n\n\n\n\n\nArticle Link:\nIn medical imaging applications, image patches are usually square regions whose size can vary from 32 × 32 pixels up to 10,000 × 10,000 pixels (256 × 256 pixels is a typical patch size\nSquare Patches: Common Practice and Benefits\n\nSquare patches are popular due to their simplicity and compatibility with standard convolutional neural network (CNN) architectures. They provide balanced feature extraction and are easy to preprocess and resize.\nStudy Reference: A study on patch-based classification using CNNs found that square patches offered efficient, balanced spatial representation for deep learning models (link.springer.com).\n\nRectangular Patches: Advantages and Considerations\n\nRectangular patches are beneficial for capturing elongated or directional structures in tissues, such as blood vessels or specific cellular arrangements. They can maximize context capture along a specific axis.\nStudy Reference: Research on adaptive patch extraction found that rectangular patches, when properly oriented, improved analysis accuracy for tissues with dominant directional features (arxiv.org).\n\nComparative Analysis\nSquare patches are easier to integrate into standard CNNs and preprocessing workflows, making them suitable for general histopathological analysis. Rectangular patches, while more complex to implement, offer better feature representation for specific tissue structures. \n\n\n\n\n\n\n\n\n\nPadding effect: (Used in Step 4)\n\n\n\n\n\n\nCNNs and Image Classification:\nConvolutional Neural Networks (CNNs) have emerged as a dominant architecture in the realm of image classification due to their capacity to capture hierarchical patterns in visual data. Their structure, characterized by convolutional layers, pooling layers, and fully connected layers, enables the effective extraction and learning of features such as edges, textures, and complex shapes. Image classification is critical in various applications, from medical imaging diagnostics to autonomous vehicle navigation and facial recognition technologies.\nRole of Padding in CNNs:\nPadding refers to the practice of adding extra pixels around the borders of an image before it is passed through a convolutional layer. This added border can be composed of zeros or other methods of pixel replication, and serves several important functions in the context of deep learning models. Padding affects how convolution operations treat the image boundaries, ensuring that every pixel, including those at the edges, receives the same level of processing as those in the center.\nTypes of Padding:\nValid Padding (No Padding):\n\nThis type processes only the valid, central pixels of the image, avoiding the addition of extra pixels. The downside of valid padding is that it reduces the spatial dimensions of the image after each convolutional layer, potentially leading to loss of information and reduced feature maps.\n\nSame Padding:\n\nSame padding involves adding pixels around the border such that the output feature map has the same width and height as the input. This type of padding ensures that spatial dimensions are maintained throughout the network, allowing for deeper architectures to operate without diminishing the feature map size.\n\nZero Padding:\n\nThis is a specific type of same padding where the border pixels added are zeros. It is used to control the size of the output while ensuring that edge pixels contribute to feature extraction.\n\nReflection Padding:\n\nReflection padding is a type of padding where the pixel values along the borders of an image are mirrored, creating a natural extension of the image edges. This method is designed to preserve edge continuity, making it particularly beneficial for tasks where accurate edge processing is essential.\nVisual Impact: The padded border reflects the nearest pixels from the original image, creating a seamless transition that better mimics natural image boundaries compared to zero padding.\nFeature Extraction: Reflection padding enhances feature extraction by preserving the original edge information, allowing convolutional filters to process these pixels as part of the existing image. This continuity leads to stronger responses and better feature learning at the borders.\nUse Cases and Benefits: Reflection padding is advantageous in scenarios where maintaining edge information is critical, such as in medical imaging or fine-grained image recognition tasks. Studies have demonstrated that using reflection padding can improve model performance, particularly for complex images where edge details carry significant information.\n\nImpact of Padding on Image Classification Performance\nPadding plays a critical role in maintaining the integrity of the data passed through CNNs and can significantly affect the model’s overall performance.\nPreservation of Spatial Dimensions:\n\nSame padding is particularly valuable because it prevents the reduction of image size across layers. This preservation is crucial when working with complex and deep networks where retaining spatial dimensions is necessary for accurate feature extraction.\n\nReduction in Information Loss:\n\nWithout padding, the edges of an image can receive less attention from convolutional layers, potentially leading to information loss. Padding mitigates this issue by providing a buffer around the image, ensuring that features at the borders are adequately processed and retained.\n\nImprovement in Feature Extraction:\n\nResearch has shown that padding enhances the network’s ability to detect edge-related features, contributing to a more holistic understanding of the input image.\n\nComputational Cost:\n\nWhile padding ensures that the output feature map size remains the same as the input, it can increase computational complexity. Larger feature maps require more memory and processing power, especially in deep networks. This can impact the overall training time and computational cost of the model.\n\nPadding References:\n\n“The Impact of Padding on Image Classification by Using Pre-trained Convolutional Neural Networks” (2019): This study investigated the effect of pre-processing, specifically padding, on image classification using pre-trained CNN models. The authors proposed a padding pre-processing pipeline that improved classification performance on challenging images without the need for re-training the model or increasing the number of CNN parameters.\n“Position, Padding and Predictions: A Deeper Look at Position Information in CNNs” (2024): This research explored how padding influences the encoding of position information in CNNs. The study revealed that zero padding drives CNNs to encode position information in their internal representations, while a lack of padding precludes position encoding. This finding suggests that padding not only affects spatial dimensions but also the way positional information is processed within the network.\n“Effects of Boundary Conditions in Fully Convolutional Networks for Spatio-Temporal Physical Modeling” (2021): This study examined various padding strategies, including reflection padding, within the context of spatio-temporal physical modeling using fully convolutional networks. The researchers found that reflection padding significantly helped in preserving edge continuity and reduced boundary artifacts, resulting in improved network generalization and accuracy. This effect was particularly notable in tasks requiring precise edge information, such as fluid dynamics simulations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT390 CMIL Classification",
    "section": "",
    "text": "Aim: The objective of this project is to identify the severity of potential eye cancer by looking at a particular eye tissue of the patient.\nIn medical terms, we want to develop a machine learning model to accurately classify Conjunctival melanocytic intraepithelial lesions (C-MIL) as per the WHO 2022 classification system. Providing a reproducible and accurate grading of C-MIL will help doctors select the most appropriate management plan for the patient.\nThe Northwestern University STAT390 Class has made the following progress on this project:\n\n\n\n\n\nStep 1: Extracting tissue slices from Whole Slice Images (WSI) using QuPath\nStep 2: Matching similar tissue slices across the different stains (H&E, Melan-A, Sox-10) for each patient when there is a correct match\nStep 3: Send matched slices to pathologist team, who will annotate H&E slice with high grade, low grade, and benign regions and send back to us\nStep 4: While the pathologist team annotates slices, orienting matched slices and apply patching across epithelium for matched slices. Currently looking at 3 different approaches to patching to find the optimal results\nLiterature Review: Researched adaptive pooling, cross view transformer, square patches (advantages, sizes, and rationale), padding effect, etc."
  },
  {
    "objectID": "step1.html",
    "href": "step1.html",
    "title": "Step 1: Slice Extraction",
    "section": "",
    "text": "Aim: To extract the tissue slices from the Whole slide images (WSI)\n\n\nMethodology\nCurrently, extracting slices is done manually. This process begins by manually importing the whole slide image into QuPath. From here, each slice is individually annotated. This is done in one of two ways. One option is to use the manual annotation tools. Most often, the brush tool is used to highlight the individual slice, and then each slice can be highlighted and exported as its own .tif file. The other option is to use the create thresholder feature (top bar -&gt; classify -&gt; pixel classification -&gt; create thresholder). The settings can be seen below (though you may need to adjust slightly to ensure whole slice is annotated). From here, you can export in the same way as before.\n\n\n\nResults\nFolder of Extracted Slices\nTracker Data of Status of Each Slice\n\n\nFuture Automation\nThis method has been explored as a future solution to fully automate the extraction of the slices from the whole slide image. Currently, the efforts have been focused elsewhere, and the automation will be further fleshed out in the future.\nThis method employs a Groovy script in QuPath to automate the identification, processing, and export of tissue regions of interest (ROIs) from Whole Slide Images (WSIs). Key scripts, including tissues_2.json, automate_export_newest.groovy, and existing_annotations.groovy, enable efficient processing and handling of diverse stain types. After setting up the project and placing scripts in their designated directories, the automate_export_newest.groovy script is executed to detect and export regions of interest (ROIs) as tissue slices, which are saved in the processed_data folder.\nThe process begins by ensuring the image is not a mask file before applying stain deconvolution to separate Hematoxylin and Eosin (H&E) stains using predefined parameters for color deconvolution. If no annotations exist, a pixel classifier (tissues_2) is applied to detect tissue regions, generating initial annotations. These annotations are refined by merging those within a set distance threshold (5000 micrometers) to consolidate closely related tissue regions, leveraging centroid-based Euclidean distance calculations. Duplicate annotations with centroids closer than 50 micrometers are identified and removed to ensure clean and accurate outputs. Finally, the refined annotations are exported as high-resolution image tiles into a specified output directory, using a customizable downsample parameter to accommodate variations in image quality. This pipeline ensures efficient and accurate tissue ROI extraction, supporting downstream analysis and quality control.\nFollow this step by step manual to extract slices.\n\n\n\n\n\n\nStep-by-Step Manual\n\n\n\n\n\nCodes required:\n\ntissues_1.json, tissues_2.json and automate_export_newest.groovy\nNew for other stain types to expedite process: existing_annotations.groovy\n\n\nSetting up in QuPath\n\nOpen a project that has all your images\nPut tissue_2.json into base_proj_dir/classifiers/pixel_classifiers (make if does not exist) Note, use tissues_2.json for most recent results (not tissues_1 but you can still try this too. tissues_2 contains broader parameters for a more sensitive model, works on more stains and images)\nPut automate_export_newest.groovy into base_project_dir/scripts (make if does not exist)\nMake sure you have an image open in QuPath interface\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; automate_export_newest.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir\n\n\nTo deal with more difficult stain types if you decide to manually annotate:\nRuns like automate_export_newest.groovy but only if you already have annotations\n\nNeed to set annotation class to “Positive” in QuPath (Annotations –&gt; Positive –&gt; Set selected and for future annotations to be auto “Positive,” press “Auto set”“)\nTo export existing annotations only, run existing_annotations.groovy\nexisting_annotations.groovy –&gt; base_project_dir/scripts\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; existing_annotations.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir\n\n\n\nTo create a new pixel classifier or modify (optional):\n\nQuPath Interface top bar –&gt; Classify –&gt; Pixel Classification –&gt; Create thresholder\nSee tissues_1.json and tissues_2.json for my parameters, and you can work from there\nSave this and then replace tissues_2 in .groovy script.\n\n\n\n\nStep 1: First pass of algorithm\nFollowing the instructions above, open your image in QuPath and run this “annotation export newest” groovy script.\n\n\n\n\n\nSelect Run, then Run For Project\n\n\n\n\n\nNote: If your automation fails while running due to a particularly large image or systematically fails on a stain type (i.e. Sheffield Sox10–most fail because reference image annotation is too large to export), you have two options: Manually annotate and export images (more on this later) Downsample an annotated area (last resort, but can successfully downsample up to a factor of 2 to match stakeholder’s desired resolution), can do this directly by changing the downsample parameter\nSelect your images to process. Not counting the mask images, I tended to process up to 20 at a time to reduce the memory load.\n\n\n\n\n\n\n\nStep 2: Analyze results and troubleshoot\nOnce you run the automation for your images, I check in QuPath directly image by image to ensure all data was properly exported. You should also check in the processed_images dir created in your Qupath project dir that no image was corrupted or too blurry. In order of manual work needed, here are the possible cases for your images. They correspond with how we dealt with and logged processing these images in the Tracker Data of Status of Each Slice\n\n\n\n\nHere are the six result cases we encountered and what to do with each one. Some require rerunning certain codes.\n\n\n\n\n\n\nResult Cases\n\n\n\n\n\nCase 1: perfect ROI identification Self-explanatory, all ROIs were successfully found and exported\nExample: Liverpool h1831023\n\n\n\n\n\nCase 2: merging Some of the region was not selected by the algorithm but belongs in the tissue sample This has to be determined across stains because some tissues might be separated in one type of stain but appear merged in another However, we don’t want to over-merge as the amount of whitespace makes matching difficult\nExample of merging: h1846151 small hanging pieces are okay to merge\n\n\n\n\n\nExample of when to not merge: h1810898B because sox10 looks similar to unmerged h&e\n\n\n\n\n\n\n\n\n\n\nThen, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory\nCase 3: deletion For any of the following types of areas, delete the annotations in QuPath: Blank images Example: Sheffield 77\n\n\n\n\n\nSplotches (shadows on the glass? blurs?)\n\n\n\n\n\nThen, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering\nCase 4: manual selection from poor selection Sometimes, the annotation region is specified correctly but with too much whitespace/unnecessary area outside Delete the original annotation, select a new region, set the class to Positive Then, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering\nExample: selecting around the hair in h2114185 h&e\n\n\n\n\n\nExample: h1845484 sox10: selection reduces the splotches’ area and prevents them from being exported extraneously\n\n\n\n\n\nCase 5: manual selection from image too large If Qupath runs out of memory when trying to run images or is stuck on a particular one (ie most of Sheffield sox10 due to large reference tissues), I created a less memory-intensive existing annotations groovy script Select each annotation region manually in QuPath, then set class as Positive Then, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering\nExample: reference tissues in most of Sheffield sox10–select actual tissue manually instead of running the algorithm–the large files like this will prevent efficient exports\n\n\n\n\n\nCase 6: not even manual selection works to export large image Try to export each annotated area at a time by selecting, selecting class → Positive, and running the “existing annotation” groovy Worst case, downsample by 2.0 factor max Then, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering\nExample: Sheffield 85 (lots of samples, junk images, and large files)\n\n\n\nThese are some ideas on how to create an API from Step 1 to Step 2 if the team decides to do so.\n\n\n\n\n\n\nIntegrating Step 1 (QuPath) into Project API\n\n\n\n\n\n\n1. Using the paquo Library\npaquo is a Python library designed specifically to interact with QuPath projects. It leverages the jpype library to seamlessly bridge Python and Java, making it possible to manipulate QuPath projects directly from Python. paquo provides native support for creating, editing, and running scripts within QuPath projects, aligning well with the goal of creating a Python-based API.\nAdvantages\n\nSimplifies Java-Python interaction for people who have little experience with Java or Groovy\nNative support for QuPath scripts and projects\nCan be integrated into current project API (steps 2-3)\n\nChallenges\n\nJVM configuration can be prone to errors\nRequires the correct QuPath version\nReferences:\n\nhttps://paquo.readthedocs.io/en/latest/\nhttps://forum.image.sc/t/paquo-read-write-qupath-projects-from-python/41892\n\n\n2. Using Python with QuPath CLI\nQuPath provides a command-line interface (CLI) that can be accessed through Python’s subprocess module. This allows Python scripts to execute Groovy-based workflows in QuPath indirectly.\nAdvantages\n\nDon’t need a JVM setup in Python\nSimple and lightweight\n\nChallenges\n\nLimited feedback from QuPath to Python\nRequires separate Groovy scripts\nReferences:\n\nhttps://www.imagescientist.com/command-line-and-python\nhttps://forum.image.sc/t/automating-qupath-pipeline-completely-using-python/72341\n\n\n3. Standalone Java Application\nA Java application can directly utilize the QuPath API to interact with projects, import images, and execute scripts. This approach bypasses Python entirely and offers complete control over QuPath’s capabilities. A Java-based solution can serve as a standalone API or backend that exposes QuPath functionalities via user-friendly interfaces (e.g., GUIs or REST endpoints).\nAdvantages\n\nDirect access to all QuPath functionalities\nFull performance optimization in Java\n\nChallenges\n\nRequires Java programming expertise\nReferences: https://forum.image.sc/t/load-project-from-a-project-file-using-qupath-java-api/63613\n\n\n\n4. Python and Java with Jython\nJython enables Python scripts to directly execute Java code. It acts as a bridge between Python and Java but is limited to Python 2.x. Jython can provide a direct way to call QuPath’s Java API from Python-like syntax, enabling API functionalities like project management and script execution.\nAdvantages\n\nDirect access to Java classes\n\nChallenges\n\nLimited to Python 2.x.\nNo support for modern Python features\nRequires Java programming expertise\nReference: https://github.com/qupath/qupath/wiki/Working-with-Python"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for the STAT390 C-MIL Classification project!"
  },
  {
    "objectID": "about.html#people",
    "href": "about.html#people",
    "title": "About",
    "section": "People",
    "text": "People\nProfessor and Project Leader: Arvind Krishna\nTeam Members:\n\nThinkers: John Olsen, Kota Suzuki, Yaelle Pierre, Jake Mead, Ryan Yi, Anna Roney\nCoders: Sari Eisen, Sharon Lin, Ryan Lach, Nathan Jung, Hannah Ma\nLiterature Surveyors: William Wang, Bennett Markinson, Allen Zhang, Alex Zhou, Jacob Muriel, Patrick Schmid, Lainey Neild\nConsultants/Organizers/Testers: Olivia Joung, Anna Deka, Margaret Pirozzolo, Jack Ouyang, Haneef Usmani, Radhika Todi\nHealth informaticians: Lucinda Hu, Dila Bitlis, Julia Nelson, Walker Frisbie, Kayla Terrelonge\n\nStakeholders: Dr. Yamini Krishna, Dr. He Zhao, Prashant Kumar"
  },
  {
    "objectID": "about.html#github-and-onedrive",
    "href": "about.html#github-and-onedrive",
    "title": "About",
    "section": "GitHub and OneDrive",
    "text": "GitHub and OneDrive\nThis github contains all the necessary code and algorithms from our progress:\nGitHub\nThis OneDrive contains all data and processed folders from our progress:\nOneDrive"
  },
  {
    "objectID": "about.html#data-overview",
    "href": "about.html#data-overview",
    "title": "About",
    "section": "Data Overview",
    "text": "Data Overview\nThe Data consists of 105 cases from 97 patients. Below are some statistics of the data:\n\n60 women (age range: 23-93 years; median 65; mean 61.5) and 37 men (age range: 31-91 years; median 68; mean 66.5)\nThe ethnic group mix comprised of the following: 88 White Caucasian, 4 Black, 2 South Asian, 1 Inuit, 1 Mixed race, and 1 Unspecified\n\nThe data corresponds to patients in the following 3 ocular oncology/pathology centers:\n\nLiverpool University Hospitals NHS Foundation Trust (Liverpool; cases from 2018 to 2021),\nRoyal Hallamshire Hospital (Sheffield; from 2011 to 2021), and\nRigshospitalet (Copenhagen; from 1996 to 2021)\n\nFor each patient, a tissue is taken from their conjunctiva. The tissue is sliced into multiple slices. Multiple slices are used so that each slice can be analyzed separately and there is more evidence to support the conclusion, i.e., the classification of the C-MIL."
  },
  {
    "objectID": "step2.html",
    "href": "step2.html",
    "title": "Step 2: Matching Slices Across Stains",
    "section": "",
    "text": "Aim: To identify and match structurally similar slices across stains for each patient.\n\n\n\n\n\n\nMethodology\nThe matching of slices is currently done manually. After the slices are extracted, the H&E, Melan-A, and SOX-10 slices are compared to see if there are any matches across all three stains. Often, these matches are very visually similar, but other times the silhouettes are fairly similar, which also makes for a match. Each patient’s data has its own folder, labeled by case number. Matched slices are loaded into folders named match1, match2, etc. and unmatched slices are loaded into folders named unmatched. The status of each case is seen below.\nTracker Data of Status of Each Slice\n\n\nResults\nExample:\nAll tissue slices from patient h2114153:\n      \nSuccessfully matched results\n  \n\n\n\n\n\n\n\nFuture Automation\nThis pipeline automates the preprocessing, matching, alignment, and patch extraction of stained tissue images to enable efficient downstream analysis. The process begins with the preprocessing phase, where images are standardized and organized. Files are renamed to a consistent format, and patients missing one or more strain types (H&E, Melan-A, SOX-10) are excluded. For patients with multiple images of the same strain, only the highest-resolution image is retained. Once cleaned, images are split into folders by patient ID to prepare for further analysis.\nThe matching phase groups corresponding images across the three stains for each patient by calculating distances between extracted contours to find optimal matches. The aligned images are saved as “matches” and further processed during the alignment phase, where images are rotated and resized to maximize overlap while maintaining consistent dimensions. A contour-based algorithm crops images to their regions of interest before alignment.\nFinally, the patch extraction phase identifies and extracts tissue patches containing both epithelium and stroma using skeletonization and gradient-based methods. Each patch is validated to ensure it contains components from all three stain types and meets quality criteria, such as having a balanced proportion of tissue and background pixels. Patches are saved in an organized structure, enabling seamless comparison across stains. The pipeline integrates error handling and modular design to ensure robustness, scalability, and adaptability to varied datasets.\nFollow this step-by-step manual to match slices. Our API is included which automates the process.\n\n\n\n\n\n\nStep-by-Step Manual (API included)\n\n\n\n\n\nTwo codes required:\n\npipeline (1).py\n\nFollow these steps to use the pipeline to generate matching slices for all patients or a specific subset:\nTo ensure this script works correctly, please follow the instructions below:\n\nRun Cara’s automation script to generate the ‘processed_images’ directory\nEnsure that each file is named the same way (with upper and lower case letters): patient ID + strain type + ROI number (separated by underscores)\nRun the script pipeline (1).py and select the ‘processed_images’ directory\n\n\nAPI\nHere is the API we created to automate this step. Using this tool will speed up the process."
  },
  {
    "objectID": "step5_previous_approach.html",
    "href": "step5_previous_approach.html",
    "title": "Automation",
    "section": "",
    "text": "As noted in the Slice Extraction and Matching steps, we have been working manually. We do, however, hope to eventually make this whole process automated through a few key programs. These include slice extraction, tissue matching, and epithelium detection."
  },
  {
    "objectID": "step5_previous_approach.html#caras-method",
    "href": "step5_previous_approach.html#caras-method",
    "title": "Automation",
    "section": "Cara’s Method",
    "text": "Cara’s Method\nAs already detailed in Step 1, Cara’s code aims to expedite the slice extraction process by automatically exporting slices from a single slide image as individual annotations and saving them into a processed_data folder, using the following code: tissues_1.json, tissues_2.json, automate_export_newest.groovy, and existing_annotations.groovy.\nHere is the documentation for Cara’s Method in case you are interested in using this algorithm now: Link\nAs previously stated above, however, we ultimately opted to extract our slices manually for better accuracy and efficiency. For future attempts, here are a few areas for improvement:\n\nMerging/Border Issues:\n\nParticularly when performed on slides with multiple slices, Cara’s code frequently either fails to merge (saves a single slice as two or more different slices) or incorrectly merges (saves two or more different slices as a single slice), requiring users to then manually adjust. On other occasions, the algorithm accurately identifies a region as a slice, yet includes too much unnecessary whitespace within the indicated annotation—again requiring manual adjustment. Potential solutions could include accounting more for the amount of whitespace in between neighboring slices and/or slice sizes (e.g. the difference in size/distance between the two respective slices in the slide images below).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse/Failed Annotations:\n\nAnother frequent issues was that the code would either incorrectly annotate a region of the slide (i.e. blank space or a shadow/blur) or would fail to annotate a clear slice. In our case, the latter issue appeared to commonly occur when dealing with slide images that only consisted of one large slice (e.g. many of the slides from Copenhagen), again requiring manual tracing and exporting instead. While manual extraction may ultimately be the more efficient option when only dealing with these large, single-slice images (as you would effectively be downloading and running code for just a single slice), it would again be helpful if the code could potentially be adjusted to better account for miscellaneous smudges/shadows on the slide."
  },
  {
    "objectID": "step5_previous_approach.html#methodology",
    "href": "step5_previous_approach.html#methodology",
    "title": "Automation",
    "section": "Methodology",
    "text": "Methodology\nThis project involves a comprehensive image processing workflow designed to segment epithelial and stromal regions from tissue images stained with different markers. The methodology combines advanced image processing techniques, including color space transformations, morphological operations, and region-based segmentation, tailored for specific stain types and locations (e.g., Liverpool or Sheffield). For each image, the script Epithelium Extraction Script.ipynb first identifies background regions using the luminance channel. The algorithm then isolates epithelia and stroma using chroma channels, applying binning techniques to detect dominant pixel intensity ranges. Morphological operations such as dilation, erosion, and small object removal refine the segmentation. Gaussian smoothing is incorporated to reduce noise and improve mask quality. Segmented regions are applied as binary masks to the original image for visualization and saved for further analysis. Parameter tuning and function selection were performed iteratively based on intermediate visualizations, ensuring high accuracy for each stain-location combination.\nAmong the six algorithms implemented, those for H&E stains exhibit the highest accuracy, followed by Melan-A and Sox-10 stains. Each algorithm is adapted to the unique characteristics of the stain and location, ensuring reliable extraction of relevant regions.\nThe segmentation process for H&E stained images leverages the luminance and chroma characteristics of the image to differentiate background, stroma, and epithelial regions effectively. Using the YCrCb color space, the luminance channel identifies the background by isolating the bin with the highest pixel count, refined further through morphological operations like hole filling and object removal. For epithelial segmentation, the red chroma channel is used, with bins adjacent to the stroma bin analyzed to isolate epithelial regions. Morphological techniques, including dilation and Gaussian smoothing, enhance the segmentation mask. The process ensures robust separation of regions, saving the results as masks and visualizations for further analysis. This approach maintains adaptability to the variable staining intensities and patterns characteristic of H&E images.\nThe methodologies for segmenting and extracting epithelium and stroma regions from tissue images stained with Melan-A and Sox-10 employ similar algorithms but adapt to the unique properties of each stain. Both methods begin by loading the images and converting them to the YCrCb color space to leverage luminance (Y) and chroma (Cr or Cb) channels for segmentation. The background is identified using a binning approach on the luminance channel, isolating the most common intensity bin. Morphological operations like object removal and hole filling refine the background mask, ensuring accurate separation of non-background elements.\nFor Melan-A stains, the red chroma channel is used to identify stroma and epithelium, leveraging bins surrounding the most frequent intensity to segment stroma and bins further offset to isolate epithelium.\nFor Sox-10 stains, the blue chroma channel is used similarly, with a tailored binning strategy to reflect the different chromatic characteristics of this stain. In both cases, stroma and epithelium masks are refined through morphological dilation, Gaussian smoothing, and removal of small objects or holes. The resulting masks are applied to the RGB images to generate segmented outputs, which are saved alongside visualizations of the input, stroma, and epithelium regions. These methods balance robustness and flexibility, ensuring segmentation quality across diverse tissue samples and staining variations.\nFollow this step-by-step manual to extract epithelia. Our API we created is included which automates the process.\n\n\n\n\n\n\nStep-by-Step Manual (API included)\n\n\n\n\n\nCodes required:\n\nEpithelium Extraction Script.ipynb\n\n\nSet the input and output directories: Make sure the paths for the input and output directories are correctly configured to point to the appropriate folders on your computer.\nRename files to include locations: Update the file names to include their locations (E.G., h2114189 melan_ROI_1 (Sheffield).tif). This step is essential because the extraction algorithm selection depends on both the stain type and location.\nRun the setup code: Execute all code cells from the purple header through the end of the file before executing the code cell below. Verify the folder object: Confirm that the folder object contains the correct tissue scans. Once verified, you may execute the code cell below.\nRun Epithelium Extraction Script.ipynb.\n\n\nAPI\nHere is the API we created to automate this step. Using this tool will speed up the process."
  },
  {
    "objectID": "step5_previous_approach.html#results",
    "href": "step5_previous_approach.html#results",
    "title": "Automation",
    "section": "Results",
    "text": "Results\nAs seen below, this program could pull out the epithelium on certain samples.\n\n\n\n\n\nHowever, the program often failed, especially on more irregular shapes as seen below.\n{fig-align=“center” width = 60%} {fig-align=“center” width = 60%}\n** The ultimate goal is to have this program work, so that all doctors have to do is push a button. However, with such inconsistent results, it is better to focus our efforts elsewhere, as we know doctors have the ability, in QuPath, to pull out slices and epithelium masks if need be.**"
  },
  {
    "objectID": "step4.html",
    "href": "step4.html",
    "title": "Step 4: Patching",
    "section": "",
    "text": "This step is happening in tandem with Step 3 (slice annotation)\nAim: To determine whether a fixed patch size can be used effectively for patching. We propose running a patching algorithm on one stain and analysing the distribution of patches it generates. If a large majority of the patches are a specific size, that size can be considered the optimal. Since the distribution of patches will be the same across stains, the optimal patch size found can be used across all three stains."
  },
  {
    "objectID": "step4.html#patching-questions",
    "href": "step4.html#patching-questions",
    "title": "Step 4: Patching",
    "section": "Patching Questions",
    "text": "Patching Questions\n\nHow many distinct patch sizes should we consider? Should we fit patches on every tissue slice to visualise the distribution? \nDoes having more patches make the ML model more complex? COnsequently, shold we try to minimize the number of distint patch sizes we use? \nIf adaptive pooling can be used effectively to handle images of different sizes (i.e. handle distinct patch sizes) can a ML model distinctly identify patterns in small patches or large patches?"
  },
  {
    "objectID": "step4.html#results",
    "href": "step4.html#results",
    "title": "Step 4: Patching",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "step4.html#alyssas-code",
    "href": "step4.html#alyssas-code",
    "title": "Step 4: Patching",
    "section": "Alyssa’s Code",
    "text": "Alyssa’s Code\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nPatches grow until they cover a sufficient region of the epithelium, which allows for better adaptability across different epithelium widths  This also allows for some overlap between patches, which helps create a balance for ensuring coverage and not missing any key spots without too much duplication.\nThis method incrementally increases patches using very small step sizes, making code time consuming (likely why this algorithm does not work as effectively with larger, thicker tissues (e.g. the large Copenhagen slices))  This algorithm also depends on color thresholds to identify tissue regions, which may lead to inconsistencies or require frequent adjustments due to variations. - For example, the black and white mask isolates the epithelium from the stroma and background.  - Stain type/image quality/lighting could affect which part is labeled as epithelium vs. stroma vs. background, which could then lead to melanocytes being cut out/not captured in patches.  Differences in color characteristics, staining protocols, lighting, or image quality could create challenges for reliable application.  The portion of the algorithm that checks for overlap rejects new patches if there is more than 20% overlap with an existing patch. This percentage could be harder to maintain in narrow regions with smaller patches and/or anywhere small overlaps might be unavoidable for complete coverage."
  },
  {
    "objectID": "step4.html#aryamans-code",
    "href": "step4.html#aryamans-code",
    "title": "Step 4: Patching",
    "section": "Aryaman’s Code",
    "text": "Aryaman’s Code\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nThe patch will be generated from the points along the centerline, ensuring that the epithelium is well-positioned within the square patch. Consistency across patches since the points are always centralized within the epithelial region. \nCode checking if the patch contains both stroma and epithelium is also time consuming, as it requires manual entry  If patch doesn’t contain both, you have to go back and try another one."
  },
  {
    "objectID": "step4.html#elis-code",
    "href": "step4.html#elis-code",
    "title": "Step 4: Patching",
    "section": "Eli’s Code",
    "text": "Eli’s Code\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\n- Advantage 1- Advantage 2\n- Disadvantage 1- Disadvantage 2"
  },
  {
    "objectID": "step4.html#aryamans-algorithm",
    "href": "step4.html#aryamans-algorithm",
    "title": "Step 4: Patching",
    "section": "Aryaman’s algorithm",
    "text": "Aryaman’s algorithm\nWe use edge detection and find the slope between two points on the edge. Using that slope, we create one edge of the square. This way, the square is tilted to match up with the angle of the sample.  Upsides: There is already code for that edge detection and finding the edge of the square.  Downsides: The samples are very irregular, so, when the edge is extended, a patch could cut off a lot of epithelium. That cut off epithelium could then be picked up by other patches, but there might be inefficiency in the patching this way. Also, see downsides of Aryaman’s whole algorithm."
  },
  {
    "objectID": "step4.html#alyssas-algorithm",
    "href": "step4.html#alyssas-algorithm",
    "title": "Step 4: Patching",
    "section": "Alyssa’s algorithm",
    "text": "Alyssa’s algorithm\nPatches grow incrementally until they cover a sufficient region of the epithelium, allowing the algorithm to adapt better to varying epithelium widths. The method also permits some overlap between patches, striking a balance between thorough coverage and avoiding excessive duplication.  Upsides: This adaptability across different epithelium widths improves coverage, and the overlap between patches helps ensure no critical regions are missed while avoiding excessive redundancy.  Downsides: Small step sizes can computationally expensive, particularly when dealing with larger tissues, such as the Copenhagen dataset with single large slices per stain. The algorithm’s reliance on color thresholds to identify tissue regions can lead to inconsistencies or require frequent adjustments due to variations in stain type, image quality, or lighting."
  },
  {
    "objectID": "step4.html#different-tilting-algorithm",
    "href": "step4.html#different-tilting-algorithm",
    "title": "Step 4: Patching",
    "section": "Different tilting algorithm:",
    "text": "Different tilting algorithm:\nWe pick a random patch. Next, we identify the stroma and the outside of the sample. We treat the pixels of the stroma and the outside of the sample as points. Using these points, we run a regression model to draw the line that most efficiently crosses the epithelium. Now, we will tilt the patch so that two edges are perpendicular and two edges are parallel to that regression line. This will create a patch that follows the contour of the epithelium. Below is a visual representation of this process.\n\n\n\n\n\nUpsides: this method is not very computationally expensive, especially with the epithelium mask that we have for each sample already.  Downsides: When tilting an existing patch, we cannot ensure that the patch still includes the entire epithelium unless we also resize the patch.  Note: This separates the tilting from the patching process. While this is not necessarily a negative, we don’t know which patching algorithm works best with this method yet. It adds another step to the process."
  },
  {
    "objectID": "step4.html#patch-design-and-optimization-qna",
    "href": "step4.html#patch-design-and-optimization-qna",
    "title": "Step 4: Patching",
    "section": "Patch Design and Optimization QnA",
    "text": "Patch Design and Optimization QnA\n\nQuestion 1: Why are square patches used in tissue analysis?\nSquare images are commonly used in tissue image analysis as they naturally fit into the grid-like structure of digital images, making it easier for algorithms to scan and analyze oatterbs, They streamline calculations for convolutional neural networks (CNNs), which process grids of data efficiently. This uniformity ensures all parts of an image are treated equally, avoiding distortions or biases that could arise from irregular shapes, i.e. rectangular patches. Additionally, square patches align with pre-trained models, saving time and effort by leveraging existing resources. They also simplify techniques like pooling, which reduces data size while preserving important features, focusing on key details without unnecessary complexity. Overall, they offer a balance of computational efficiency and accuracy.\n\n\nQuestion 2: Why is patch size in powers of 2? (e.g. 32 x 32, 256 x 256, 512 x 512, etc.) Are there alternatives?\nplaceholder\n\n\nQuestion 3: How do researchers determine the optimal number of patches?\nplaceholder"
  }
]