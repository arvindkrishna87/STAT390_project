[
  {
    "objectID": "step5.html",
    "href": "step5.html",
    "title": "Step 5: TBD",
    "section": "",
    "text": "Aim: We will find this out soon!\nMethodology:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2024 STAT390 (CMIL Classification)",
    "section": "",
    "text": "Aim: The objective of the project is to develop a machine learning model to accurately classify Conjunctival melanocytic intraepithelial lesions (C-MIL) as per the WHO 2022 classification system. In layman’s terms, we want to identify the severity of potential eye cancer by looking at a particular eye tissue of the patient.\nThe motivation behind the project is to provide a reproducible and accurate grading of C-MIL so that the most appropriate management plan for the patient can be developed. By improving the precision of lesion classification, our model has the potential to enhance patient outcomes and optimize treatment strategies\nThe Northwestern University STAT390 Class of Fall 2024 has made the following progress on this project:\n\nExtracting each tissue from the patient data using QuPath\nMatching tissue slices across the different stains (H&E, Melan-A, Sox-10) to each patient if there is a correct match\nDetecting and extracting only the epithelium for each tissue\nPatching certain regions of each epithelium for further examination\nTBD"
  },
  {
    "objectID": "step1.html",
    "href": "step1.html",
    "title": "Step 1: Slice extraction",
    "section": "",
    "text": "Aim: To extract the tissue slices from the WSI (Whole slide images)\nMethodology: QuPath scripting files to automate tissue extraction"
  },
  {
    "objectID": "step1.html#setting-up-in-qupath",
    "href": "step1.html#setting-up-in-qupath",
    "title": "Step 1: Slice extraction",
    "section": "Setting up in QuPath",
    "text": "Setting up in QuPath\nImages in a QuPath Project –&gt; processed_data/tissue_image.tif Contains tissues_1.json, tissues_2.json and automate_export_newest.groovy New for other stain types to expedite process: existing_annotations.groovy\n\nOpen a project that has all your images\nPut tissue_2.json into base_proj_dir/classifiers/pixel_classifiers (make if does not exist) Note, use tissues_2.json for most recent results (not tissues_1 but you can still try this too. tissues_2 contains broader parameters for a more sensitive model, works on more stains and images)\nPut automate_export_newest.groovy into base_project_dir/scripts (make if does not exist)\nMake sure you have an image open in QuPath interface\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; automate_export_newest.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir"
  },
  {
    "objectID": "step1.html#to-deal-with-more-difficult-stain-types-if-you-decide-to-manually-annotate",
    "href": "step1.html#to-deal-with-more-difficult-stain-types-if-you-decide-to-manually-annotate",
    "title": "Step 1: Slice extraction",
    "section": "To deal with more difficult stain types if you decide to manually annotate:",
    "text": "To deal with more difficult stain types if you decide to manually annotate:\n\nRuns like automate_export_newest.groovy but only if you already have annotations\n\nNeed to set annotation class to “Positive” in QuPath (Annotations –&gt; Positive –&gt; Set selected and for future annotations to be auto “Positive,” press “Auto set”“)\nTo export existing annotations only, run existing_annotations.groovy\nexisting_annotations.groovy –&gt; base_project_dir/scripts\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; existing_annotations.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir"
  },
  {
    "objectID": "step1.html#to-create-a-new-pixel-classifier-or-modify-optional",
    "href": "step1.html#to-create-a-new-pixel-classifier-or-modify-optional",
    "title": "Step 1: Slice extraction",
    "section": "To create a new pixel classifier or modify (optional):",
    "text": "To create a new pixel classifier or modify (optional):\n\nQuPath Interface top bar –&gt; Classify –&gt; Pixel Classification –&gt; Create thresholder\nSee tissues_1.json and tissues_2.json for my parameters, and you can work from there\nSave this and then replace “tissues_2” in .groovy script."
  },
  {
    "objectID": "step1.html#step-1-first-pass-of-algorithm",
    "href": "step1.html#step-1-first-pass-of-algorithm",
    "title": "Step 1: Slice extraction",
    "section": "Step 1: First pass of algorithm",
    "text": "Step 1: First pass of algorithm\nFollowing the instructions above, open your image in QuPath and run this “annotation export newest” groovy script.\n\nSelect Run, then Run For Project\n\nNote: If your automation fails while running due to a particularly large image or systematically fails on a stain type (ie Sheffield Sox10–most fail because reference image annotation is too large to export), you have two options: Manually annotate and export images (more on this later) Downsample an annotated area (last resort, but can successfully downsample up to a factor of 2 to match stakeholder’s desired resolution), can do this directly by changing the downsample parameter\nSelect your images to process. Not counting the mask images, I tended to process up to 20 at a time to reduce the memory load."
  },
  {
    "objectID": "step1.html#step-2-analyze-results-and-troubleshoot",
    "href": "step1.html#step-2-analyze-results-and-troubleshoot",
    "title": "Step 1: Slice extraction",
    "section": "Step 2: Analyze results and troubleshoot",
    "text": "Step 2: Analyze results and troubleshoot\nOnce you run the automation for your images, I check in QuPath directly image by image to ensure all data was properly exported. You should also check in the processed_images dir created in your Qupath project dir that no image was corrupted or too blurry. In order of manual work needed, here are the possible cases for your images. They correspond with how we dealt with and logged processing these images in the Tracker Data of Status of Each Slice"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for the Fall 2024 STAT390 project!\nProfessor and Project Leader: Arvind Krishna\nTeam Members:\nMarianne Cano, Lauren Gomez, Rayyana Hassan, Baron Patterson, Cara Chang, Eli Nacar, Kenny Yeon, Luna Nguyen, Teresa Pan, Emma Drisko, Ally Bardas, Diego Goldfrank, Andrew Luke, Maddy Gallagher, Sarah Abdulwahid, Noah Schulhof, Christina Tzavara, Faith Cramer, Aryaman Chawla, Parth Patel, Atharva Weling, Kyla Bruno, Kevin Li, Alyssa Shou, Annie Xia, Alex Devorsetz, Kelly Meng\nStakeholders: Dr. Yamini Krishna, Dr. He Zhao, Prashant Kumar\nLink to connect to our Github link to connect to our API\nData overview\nData Source\nData consists of 105 cases from 97 patients. Below are some statistics of the data:\n\n60 women (age range: 23-93 years; median 65; mean 61.5) and 37 men (age range: 31-91 years; median 68; mean 66.5)\nThe ethnic group mix comprised of the following: 88 White Caucasian, 4 Black, 2 South Asian, 1 Inuit, 1 Mixed race, and 1 Unspecified\n\nThe data corresponds to patients in the following 3 ocular oncology/pathology centers:\n\nLiverpool University Hospitals NHS Foundation Trust (Liverpool; cases from 2018 to 2021),\nRoyal Hallamshire Hospital (Sheffield; from 2011 to 2021), and\nRigshospitalet (Copenhagen; from 1996 to 2021)\n\nFor each patient, a tissue is taken from their conjunctiva. The tissue is sliced into multiple slices. Multiple slices are used so that each slice can be analyzed separately and there is more evidence to support the conclusion, i.e., the classification of the C-MIL."
  },
  {
    "objectID": "step4.html",
    "href": "step4.html",
    "title": "Step 4: Patching",
    "section": "",
    "text": "Aim: To create boxes in the same matched epithelia to investigate further.\nMethodology:"
  },
  {
    "objectID": "step4.html#set-up",
    "href": "step4.html#set-up",
    "title": "Step 4: Patching",
    "section": "Set Up",
    "text": "Set Up"
  },
  {
    "objectID": "step4.html#results",
    "href": "step4.html#results",
    "title": "Step 4: Patching",
    "section": "Results",
    "text": "Results\nImages for Alyssa’s code"
  },
  {
    "objectID": "step3.html",
    "href": "step3.html",
    "title": "Step 3: Epithelium Detection",
    "section": "",
    "text": "Aim: To detect and extract the epithelium in each slice.\nMethodology:"
  },
  {
    "objectID": "step3.html#set-up",
    "href": "step3.html#set-up",
    "title": "Step 3: Epithelium Detection",
    "section": "Set Up",
    "text": "Set Up"
  },
  {
    "objectID": "step3.html#results",
    "href": "step3.html#results",
    "title": "Step 3: Epithelium Detection",
    "section": "Results",
    "text": "Results\nImages for Kevin’s updated Prashant code."
  },
  {
    "objectID": "step2.html",
    "href": "step2.html",
    "title": "Step 2: Matching slices across stains",
    "section": "",
    "text": "Aim: To identify structurally similar slices across stains.\nMethodology:\nTwo codes required:\nMatching Algorithm Evaluation Tracker"
  },
  {
    "objectID": "step2.html#step-by-step-manual",
    "href": "step2.html#step-by-step-manual",
    "title": "Step 2: Matching slices across stains",
    "section": "Step-by-Step Manual",
    "text": "Step-by-Step Manual\nFollow these steps to use the pipeline to generate matching slices for all patients or a specific subset:\nPrepare Image Data: Place .tif image files in the designated folder (e.g., processed_images). Ensure that each patient has images for all three stains (H&E, Melanin, Sox10).\nChoose Full or Subset Processing:\nFor All Patients: Place all images in the same processed_images directory. For a Subset of Patients: Separate patient image files into their own folders within processed_images (e.g., processed_images/patient1, processed_images/patient2). Run the Pipeline:\nRun match_pipeline.py. A GUI will prompt you to select the directory containing the images. Open NoahsMatching.ipynb in Jupyter Notebook and execute cells in sequence. Process and Review Matches: Upon execution, the pipeline will preprocess, extract contours, and match images, saving results in the matches folder. Images will be organized by patient, with matched slices grouped in subfolders."
  },
  {
    "objectID": "step2.html#results",
    "href": "step2.html#results",
    "title": "Step 2: Matching slices across stains",
    "section": "Results",
    "text": "Results\nFor 1 patient, show all the slices. then Show images of the matching slices it chose"
  }
]