[
  {
    "objectID": "litreview.html",
    "href": "litreview.html",
    "title": "Literature Review",
    "section": "",
    "text": "Adaptive Pooling: (Used in Step 4)\n\n\n\n\n\n\nLarge Batch and Patch Size Training for Medical Image Segmentation:\nArticle Link\n\nThis article highlights the performance improvements gained by adjusting patch and batch sizes, suggesting that larger patches improve segmentation accuracy up to a certain limit, beyond which performance plateaus or even decreases.\nAdaptive pooling could be used to create variable patch sizes within a single image, enabling finer details in complex areas and larger patches for broader context in simpler areas.\nBy using adaptive pooling, you could manage patch sizes more dynamically, potentially overcoming the plateau effect observed with fixed large patch sizes.\nThis would ensure that each region of the image is analyzed at an optimal scale, leading to potentially higher accuracy without the drawbacks of uniform large patches.\n\n\n\n\n\n\n\n\n\n\nCross View Transformer: (Used in Step 4)\n\n\n\n\n\n\nPurpose: Cross-view transformers are ideal for aligning images from different modalities (or stains) without exact pixel-level registration. They enable spatial feature alignment through learned transformations, which is helpful when the images are stained differently and thus might not naturally align perfectly.\nBenefit: Using this alignment allows our algorithm to recognize similar regions across stains, orienting them correctly and enabling the same-region patches to be matched across the stains. The algorithm would learn to map similar features across images without requiring identical sizes, which suits our no-resize constraint.\nHow to Implement:\n\nInput the Three Stains: For each patient’s tissue sample, create multi-resolution representations for each of the three stains (sox10, melana, and H&E).\nAlign Using Cross-View Transformers: Apply cross-view alignment to identify similar regions across stains by mapping corresponding features. This step ensures that the patches you sample come from the same tissue regions across the three stains.\nExtract Aligned Patches: Use the aligned regions to sample patches that are similarly oriented and positioned, ensuring consistency.\nApply Controlled Padding (if Needed): Pad the patches to make them uniform in size, allowing them to be used in the classification model.\n\nSummary: Ultimately, the multi-resolution network will capture different levels of detail from each stain, ensuring that both fine cellular structures and broader tissue architectures are represented. This helps to make the model robust to slight size or scale variations in patches. The cross-view alignment will allow our algorithm to “match” regions across different stains based on learned features rather than exact spatial coordinates. This is especially helpful when we have patches of different stains that aren’t perfectly aligned but should correspond to the same tissue region.\n\n\n\n\n\n\n\n\n\nAdvantages of square patches: (Used in step 4)\n\n\n\n\n\n\nArticle Link:\nIn medical imaging applications, image patches are usually square regions whose size can vary from 32 × 32 pixels up to 10,000 × 10,000 pixels (256 × 256 pixels is a typical patch size\nSquare Patches: Common Practice and Benefits\n\nSquare patches are popular due to their simplicity and compatibility with standard convolutional neural network (CNN) architectures. They provide balanced feature extraction and are easy to preprocess and resize.\nStudy Reference: A study on patch-based classification using CNNs found that square patches offered efficient, balanced spatial representation for deep learning models (link.springer.com).\n\nRectangular Patches: Advantages and Considerations\n\nRectangular patches are beneficial for capturing elongated or directional structures in tissues, such as blood vessels or specific cellular arrangements. They can maximize context capture along a specific axis.\nStudy Reference: Research on adaptive patch extraction found that rectangular patches, when properly oriented, improved analysis accuracy for tissues with dominant directional features (arxiv.org).\n\nComparative Analysis\nSquare patches are easier to integrate into standard CNNs and preprocessing workflows, making them suitable for general histopathological analysis. Rectangular patches, while more complex to implement, offer better feature representation for specific tissue structures. \n\n\n\n\n\n\n\n\n\nPadding effect: (Used in Step 4)\n\n\n\n\n\n\nCNNs and Image Classification:\nConvolutional Neural Networks (CNNs) have emerged as a dominant architecture in the realm of image classification due to their capacity to capture hierarchical patterns in visual data. Their structure, characterized by convolutional layers, pooling layers, and fully connected layers, enables the effective extraction and learning of features such as edges, textures, and complex shapes. Image classification is critical in various applications, from medical imaging diagnostics to autonomous vehicle navigation and facial recognition technologies.\nRole of Padding in CNNs:\nPadding refers to the practice of adding extra pixels around the borders of an image before it is passed through a convolutional layer. This added border can be composed of zeros or other methods of pixel replication, and serves several important functions in the context of deep learning models. Padding affects how convolution operations treat the image boundaries, ensuring that every pixel, including those at the edges, receives the same level of processing as those in the center.\nTypes of Padding:\nValid Padding (No Padding):\n\nThis type processes only the valid, central pixels of the image, avoiding the addition of extra pixels. The downside of valid padding is that it reduces the spatial dimensions of the image after each convolutional layer, potentially leading to loss of information and reduced feature maps.\n\nSame Padding:\n\nSame padding involves adding pixels around the border such that the output feature map has the same width and height as the input. This type of padding ensures that spatial dimensions are maintained throughout the network, allowing for deeper architectures to operate without diminishing the feature map size.\n\nZero Padding:\n\nThis is a specific type of same padding where the border pixels added are zeros. It is used to control the size of the output while ensuring that edge pixels contribute to feature extraction.\n\nReflection Padding:\n\nReflection padding is a type of padding where the pixel values along the borders of an image are mirrored, creating a natural extension of the image edges. This method is designed to preserve edge continuity, making it particularly beneficial for tasks where accurate edge processing is essential.\nVisual Impact: The padded border reflects the nearest pixels from the original image, creating a seamless transition that better mimics natural image boundaries compared to zero padding.\nFeature Extraction: Reflection padding enhances feature extraction by preserving the original edge information, allowing convolutional filters to process these pixels as part of the existing image. This continuity leads to stronger responses and better feature learning at the borders.\nUse Cases and Benefits: Reflection padding is advantageous in scenarios where maintaining edge information is critical, such as in medical imaging or fine-grained image recognition tasks. Studies have demonstrated that using reflection padding can improve model performance, particularly for complex images where edge details carry significant information.\n\nImpact of Padding on Image Classification Performance\nPadding plays a critical role in maintaining the integrity of the data passed through CNNs and can significantly affect the model’s overall performance.\nPreservation of Spatial Dimensions:\n\nSame padding is particularly valuable because it prevents the reduction of image size across layers. This preservation is crucial when working with complex and deep networks where retaining spatial dimensions is necessary for accurate feature extraction.\n\nReduction in Information Loss:\n\nWithout padding, the edges of an image can receive less attention from convolutional layers, potentially leading to information loss. Padding mitigates this issue by providing a buffer around the image, ensuring that features at the borders are adequately processed and retained.\n\nImprovement in Feature Extraction:\n\nResearch has shown that padding enhances the network’s ability to detect edge-related features, contributing to a more holistic understanding of the input image.\n\nComputational Cost:\n\nWhile padding ensures that the output feature map size remains the same as the input, it can increase computational complexity. Larger feature maps require more memory and processing power, especially in deep networks. This can impact the overall training time and computational cost of the model.\n\nPadding References:\n\n“The Impact of Padding on Image Classification by Using Pre-trained Convolutional Neural Networks” (2019): This study investigated the effect of pre-processing, specifically padding, on image classification using pre-trained CNN models. The authors proposed a padding pre-processing pipeline that improved classification performance on challenging images without the need for re-training the model or increasing the number of CNN parameters.\n“Position, Padding and Predictions: A Deeper Look at Position Information in CNNs” (2024): This research explored how padding influences the encoding of position information in CNNs. The study revealed that zero padding drives CNNs to encode position information in their internal representations, while a lack of padding precludes position encoding. This finding suggests that padding not only affects spatial dimensions but also the way positional information is processed within the network.\n“Effects of Boundary Conditions in Fully Convolutional Networks for Spatio-Temporal Physical Modeling” (2021): This study examined various padding strategies, including reflection padding, within the context of spatio-temporal physical modeling using fully convolutional networks. The researchers found that reflection padding significantly helped in preserving edge continuity and reduced boundary artifacts, resulting in improved network generalization and accuracy. This effect was particularly notable in tasks requiring precise edge information, such as fluid dynamics simulations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2024 STAT390 (CMIL Classification)",
    "section": "",
    "text": "Aim: The objective of this project is to identify the severity of potential eye cancer by looking at a particular eye tissue of the patient.\nIn medical terms, we want to develop a machine learning model to accurately classify Conjunctival melanocytic intraepithelial lesions (C-MIL) as per the WHO 2022 classification system. Providing a reproducible and accurate grading of C-MIL will help doctors select the most appropriate management plan for the patient.\nThe Northwestern University STAT390 Class of Fall 2024 has made the following progress on this project:\n\n\n\n\n\nStep 1: Extracting tissue slices from Whole Slice Images (WSI) using QuPath\nStep 2: Matching similar tissue slices across the different stains (H&E, Melan-A, Sox-10) for each patient when there is a correct match\nStep 3: Detecting and extracting the epithelium and stroma for each tissue slice. Send to pathologist team\n\nPathologist Team will annotate H&E slice with high grade, low grade, and benign regions and send back to us\n\nStep 4: Orienting matched slices and apply patching across epithelium for matched slices\nLiterature Review: Researched adaptive pooling, cross view transformer, advantages of square patches, padding effect, etc."
  },
  {
    "objectID": "step1.html",
    "href": "step1.html",
    "title": "Step 1: Slice extraction",
    "section": "",
    "text": "Aim: To extract the tissue slices from the Whole slide images (WSI)\n\n\nMethodology\nThis step employs QuPath and custom Groovy scripts to automate the extraction of tissue regions from Whole Slide Images (WSIs). Key scripts, including tissues_2.json, automate_export_newest.groovy, and existing_annotations.groovy, enable efficient processing and handling of diverse stain types. After setting up the project and placing scripts in their designated directories, the automate_export_newest.groovy script is executed to detect and export regions of interest (ROIs) as tissue slices, which are saved in the processed_data folder. The process includes error handling for large or challenging images, offering manual annotation and adjustments when necessary. Exported data is reviewed to ensure quality, with issues like merging, deletion, or poor selection addressed using targeted corrections or downsampling, ensuring consistent and accurate tissue extraction.\nFollow this step by step manual to extract slices:\n\n\n\n\n\n\nStep-by-Step Manual\n\n\n\n\n\nCodes required:\n\ntissues_1.json, tissues_2.json and automate_export_newest.groovy\nNew for other stain types to expedite process: existing_annotations.groovy\n\n\nSetting up in QuPath\n\nOpen a project that has all your images\nPut tissue_2.json into base_proj_dir/classifiers/pixel_classifiers (make if does not exist) Note, use tissues_2.json for most recent results (not tissues_1 but you can still try this too. tissues_2 contains broader parameters for a more sensitive model, works on more stains and images)\nPut automate_export_newest.groovy into base_project_dir/scripts (make if does not exist)\nMake sure you have an image open in QuPath interface\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; automate_export_newest.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir\n\n\nTo deal with more difficult stain types if you decide to manually annotate:\nRuns like automate_export_newest.groovy but only if you already have annotations\n\nNeed to set annotation class to “Positive” in QuPath (Annotations –&gt; Positive –&gt; Set selected and for future annotations to be auto “Positive,” press “Auto set”“)\nTo export existing annotations only, run existing_annotations.groovy\nexisting_annotations.groovy –&gt; base_project_dir/scripts\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; existing_annotations.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir\n\n\n\nTo create a new pixel classifier or modify (optional):\n\nQuPath Interface top bar –&gt; Classify –&gt; Pixel Classification –&gt; Create thresholder\nSee tissues_1.json and tissues_2.json for my parameters, and you can work from there\nSave this and then replace tissues_2 in .groovy script.\n\n\n\n\nStep 1: First pass of algorithm\nFollowing the instructions above, open your image in QuPath and run this “annotation export newest” groovy script.\n\n\n\n\n\nSelect Run, then Run For Project\n\n\n\n\n\nNote: If your automation fails while running due to a particularly large image or systematically fails on a stain type (i.e. Sheffield Sox10–most fail because reference image annotation is too large to export), you have two options: Manually annotate and export images (more on this later) Downsample an annotated area (last resort, but can successfully downsample up to a factor of 2 to match stakeholder’s desired resolution), can do this directly by changing the downsample parameter\nSelect your images to process. Not counting the mask images, I tended to process up to 20 at a time to reduce the memory load.\n\n\n\n\n\n\n\nStep 2: Analyze results and troubleshoot\nOnce you run the automation for your images, I check in QuPath directly image by image to ensure all data was properly exported. You should also check in the processed_images dir created in your Qupath project dir that no image was corrupted or too blurry. In order of manual work needed, here are the possible cases for your images. They correspond with how we dealt with and logged processing these images in the Tracker Data of Status of Each Slice\n\n\n\n\nHere are the six result cases we experience and what to do with each one. Some require rerunning certain codes:\n\n\n\n\n\n\nResult Cases\n\n\n\n\n\nCase 1: perfect ROI identification Self-explanatory, all ROIs were successfully found and exported Example: Liverpool h1831023\n\n\n\n\n\nCase 2: merging Some of the region was not selected by the algorithm but belongs in the tissue sample This has to be determined across stains because some tissues might be separated in one type of stain but appear merged in another However, we don’t want to over-merge as the amount of whitespace makes matching difficult Example of merging: h1846151 small hanging pieces are okay to merge\n\n\n\n\n\nExample of when to not merge: h1810898B because sox10 looks similar to unmerged h&e\n\n\n\n\n\n\n\n\n\n\nThen, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory\nCase 3: deletion For any of the following types of areas, delete the annotations in QuPath: Blank images Example: Sheffield 77\n\n\n\n\n\nSplotches (shadows on the glass? blurs?)\n\n\n\n\n\nThen, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering\nCase 4: manual selection from poor selection Sometimes, the annotation region is specified correctly but with too much whitespace/unnecessary area outside Delete the original annotation, select a new region, set the class to Positive Then, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering Example: selecting around the hair in h2114185 h&e\n\n\n\n\n\nExample: h1845484 sox10: selection reduces the splotches’ area and prevents them from being exported extraneously\n\n\n\n\n\nCase 5: manual selection from image too large If Qupath runs out of memory when trying to run images or is stuck on a particular one (ie most of Sheffield sox10 due to large reference tissues), I created a less memory-intensive existing annotations groovy script Select each annotation region manually in QuPath, then set class as Positive Then, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering Example: reference tissues in most of Sheffield sox10–select actual tissue manually instead of running the algorithm–the large files like this will prevent efficient exports\n\n\n\n\n\nCase 6: not even manual selection works to export large image Try to export each annotated area at a time by selecting, selecting class → Positive, and running the “existing annotation” groovy Worst case, downsample by 2.0 factor max Then, rerun “existing annotations” groovy script to export faster and delete remaining ROIs in your file directory to ensure consistent ROI numbering Example: Sheffield 85 (lots of samples, junk images, and large files)\n\n\n\n\n\nResults\nFolder of Extracted Slices\nTracker Data of Status of Each Slice\n\n\n\n\n\n\nIntegrating Step 1 (QuPath) into Project API\n\n\n\n\n\nThese are some ideas on how to create an API from Step 1 to Step 2.\n\n1. Using the paquo Library\npaquo is a Python library designed specifically to interact with QuPath projects. It leverages the jpype library to seamlessly bridge Python and Java, making it possible to manipulate QuPath projects directly from Python. paquo provides native support for creating, editing, and running scripts within QuPath projects, aligning well with the goal of creating a Python-based API.\nAdvantages\n\nSimplifies Java-Python interaction for people who have little experience with Java or Groovy\nNative support for QuPath scripts and projects\nCan be integrated into current project API (steps 2-3)\n\nChallenges\n\nJVM configuration can be prone to errors\nRequires the correct QuPath version\nReferences:\n\nhttps://paquo.readthedocs.io/en/latest/\nhttps://forum.image.sc/t/paquo-read-write-qupath-projects-from-python/41892\n\n\n2. Using Python with QuPath CLI\nQuPath provides a command-line interface (CLI) that can be accessed through Python’s subprocess module. This allows Python scripts to execute Groovy-based workflows in QuPath indirectly.\nAdvantages\n\nDon’t need a JVM setup in Python\nSimple and lightweight\n\nChallenges\n\nLimited feedback from QuPath to Python\nRequires separate Groovy scripts\nReferences:\n\nhttps://www.imagescientist.com/command-line-and-python\nhttps://forum.image.sc/t/automating-qupath-pipeline-completely-using-python/72341\n\n\n3. Standalone Java Application\nA Java application can directly utilize the QuPath API to interact with projects, import images, and execute scripts. This approach bypasses Python entirely and offers complete control over QuPath’s capabilities. A Java-based solution can serve as a standalone API or backend that exposes QuPath functionalities via user-friendly interfaces (e.g., GUIs or REST endpoints).\nAdvantages\n\nDirect access to all QuPath functionalities\nFull performance optimization in Java\n\nChallenges\n\nRequires Java programming expertise\nReferences: https://forum.image.sc/t/load-project-from-a-project-file-using-qupath-java-api/63613\n\n\n\n4. Python and Java with Jython\nJython enables Python scripts to directly execute Java code. It acts as a bridge between Python and Java but is limited to Python 2.x. Jython can provide a direct way to call QuPath’s Java API from Python-like syntax, enabling API functionalities like project management and script execution.\nAdvantages\n\nDirect access to Java classes\n\nChallenges\n\nLimited to Python 2.x.\nNo support for modern Python features\nRequires Java programming expertise\nReference: https://github.com/qupath/qupath/wiki/Working-with-Python"
  },
  {
    "objectID": "Step1API.html",
    "href": "Step1API.html",
    "title": "Step 1 API Recommendations",
    "section": "",
    "text": "These are some ideas on how to create an API from Step 1 to Step 2.\n\n\npaquo is a Python library designed specifically to interact with QuPath projects. It leverages the jpype library to seamlessly bridge Python and Java, making it possible to manipulate QuPath projects directly from Python. paquo provides native support for creating, editing, and running scripts within QuPath projects, aligning well with the goal of creating a Python-based API.\nAdvantages\n\nSimplifies Java-Python interaction for people who have little experience with Java or Groovy\nNative support for QuPath scripts and projects\nCan be integrated into current project API (steps 2-3)\n\nChallenges\n\nJVM configuration can be prone to errors\nRequires the correct QuPath version\nReferences:\n\nhttps://paquo.readthedocs.io/en/latest/\nhttps://forum.image.sc/t/paquo-read-write-qupath-projects-from-python/41892\n\n\n\nQuPath provides a command-line interface (CLI) that can be accessed through Python’s subprocess module. This allows Python scripts to execute Groovy-based workflows in QuPath indirectly.\nAdvantages\n\nDon’t need a JVM setup in Python\nSimple and lightweight\n\nChallenges\n\nLimited feedback from QuPath to Python\nRequires separate Groovy scripts\nReferences:\n\nhttps://www.imagescientist.com/command-line-and-python\nhttps://forum.image.sc/t/automating-qupath-pipeline-completely-using-python/72341\n\n\n\nA Java application can directly utilize the QuPath API to interact with projects, import images, and execute scripts. This approach bypasses Python entirely and offers complete control over QuPath’s capabilities. A Java-based solution can serve as a standalone API or backend that exposes QuPath functionalities via user-friendly interfaces (e.g., GUIs or REST endpoints).\nAdvantages\n\nDirect access to all QuPath functionalities\nFull performance optimization in Java\n\nChallenges\n\nRequires Java programming expertise\nReferences: https://forum.image.sc/t/load-project-from-a-project-file-using-qupath-java-api/63613\n\n\n\n\nJython enables Python scripts to directly execute Java code. It acts as a bridge between Python and Java but is limited to Python 2.x. Jython can provide a direct way to call QuPath’s Java API from Python-like syntax, enabling API functionalities like project management and script execution.\nAdvantages\n\nDirect access to Java classes\n\nChallenges\n\nLimited to Python 2.x.\nNo support for modern Python features\nRequires Java programming expertise\nReference: https://github.com/qupath/qupath/wiki/Working-with-Python"
  },
  {
    "objectID": "Step1API.html#step-1-integrating-qupath-into-project-api",
    "href": "Step1API.html#step-1-integrating-qupath-into-project-api",
    "title": "Step 1 API Recommendations",
    "section": "",
    "text": "These are some ideas on how to create an API from Step 1 to Step 2.\n\n\npaquo is a Python library designed specifically to interact with QuPath projects. It leverages the jpype library to seamlessly bridge Python and Java, making it possible to manipulate QuPath projects directly from Python. paquo provides native support for creating, editing, and running scripts within QuPath projects, aligning well with the goal of creating a Python-based API.\nAdvantages\n\nSimplifies Java-Python interaction for people who have little experience with Java or Groovy\nNative support for QuPath scripts and projects\nCan be integrated into current project API (steps 2-3)\n\nChallenges\n\nJVM configuration can be prone to errors\nRequires the correct QuPath version\nReferences:\n\nhttps://paquo.readthedocs.io/en/latest/\nhttps://forum.image.sc/t/paquo-read-write-qupath-projects-from-python/41892\n\n\n\nQuPath provides a command-line interface (CLI) that can be accessed through Python’s subprocess module. This allows Python scripts to execute Groovy-based workflows in QuPath indirectly.\nAdvantages\n\nDon’t need a JVM setup in Python\nSimple and lightweight\n\nChallenges\n\nLimited feedback from QuPath to Python\nRequires separate Groovy scripts\nReferences:\n\nhttps://www.imagescientist.com/command-line-and-python\nhttps://forum.image.sc/t/automating-qupath-pipeline-completely-using-python/72341\n\n\n\nA Java application can directly utilize the QuPath API to interact with projects, import images, and execute scripts. This approach bypasses Python entirely and offers complete control over QuPath’s capabilities. A Java-based solution can serve as a standalone API or backend that exposes QuPath functionalities via user-friendly interfaces (e.g., GUIs or REST endpoints).\nAdvantages\n\nDirect access to all QuPath functionalities\nFull performance optimization in Java\n\nChallenges\n\nRequires Java programming expertise\nReferences: https://forum.image.sc/t/load-project-from-a-project-file-using-qupath-java-api/63613\n\n\n\n\nJython enables Python scripts to directly execute Java code. It acts as a bridge between Python and Java but is limited to Python 2.x. Jython can provide a direct way to call QuPath’s Java API from Python-like syntax, enabling API functionalities like project management and script execution.\nAdvantages\n\nDirect access to Java classes\n\nChallenges\n\nLimited to Python 2.x.\nNo support for modern Python features\nRequires Java programming expertise\nReference: https://github.com/qupath/qupath/wiki/Working-with-Python"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for the Fall 2024 STAT390 project!"
  },
  {
    "objectID": "about.html#people",
    "href": "about.html#people",
    "title": "About",
    "section": "People",
    "text": "People\nProfessor and Project Leader: Arvind Krishna\nTeam Members:\nMarianne Cano, Lauren Gomez, Rayyana Hassan, Baron Patterson, Cara Chang, Eli Nacar, Kenny Yeon, Luna Nguyen, Teresa Pan, Emma Drisko, Ally Bardas, Diego Goldfrank, Andrew Luke, Maddy Gallagher, Sarah Abdulwahid, Noah Schulhof, Christina Tzavara, Faith Cramer, Aryaman Chawla, Parth Patel, Atharva Weling, Kyla Bruno, Kevin Li, Alyssa Shou, Annie Xia, Alex Devorsetz, Kelly Meng\nStakeholders: Dr. Yamini Krishna, Dr. He Zhao, Prashant Kumar"
  },
  {
    "objectID": "about.html#github-and-onedrive",
    "href": "about.html#github-and-onedrive",
    "title": "About",
    "section": "GitHub and OneDrive",
    "text": "GitHub and OneDrive\nThis github contains all the necessary code and algorithms from our progress:\nGitHub\nThis OneDrive contains all data and processed folders from our progress:\nOneDrive"
  },
  {
    "objectID": "about.html#data-overview",
    "href": "about.html#data-overview",
    "title": "About",
    "section": "Data overview",
    "text": "Data overview\nThe Data consists of 105 cases from 97 patients. Below are some statistics of the data:\n\n60 women (age range: 23-93 years; median 65; mean 61.5) and 37 men (age range: 31-91 years; median 68; mean 66.5)\nThe ethnic group mix comprised of the following: 88 White Caucasian, 4 Black, 2 South Asian, 1 Inuit, 1 Mixed race, and 1 Unspecified\n\nThe data corresponds to patients in the following 3 ocular oncology/pathology centers:\n\nLiverpool University Hospitals NHS Foundation Trust (Liverpool; cases from 2018 to 2021),\nRoyal Hallamshire Hospital (Sheffield; from 2011 to 2021), and\nRigshospitalet (Copenhagen; from 1996 to 2021)\n\nFor each patient, a tissue is taken from their conjunctiva. The tissue is sliced into multiple slices. Multiple slices are used so that each slice can be analyzed separately and there is more evidence to support the conclusion, i.e., the classification of the C-MIL."
  },
  {
    "objectID": "step4.html",
    "href": "step4.html",
    "title": "Step 4: Patching",
    "section": "",
    "text": "Aim: Applies horizontal/vertical patching across the matched and oriented epithelia."
  },
  {
    "objectID": "step4.html#pathologist-team-annotations",
    "href": "step4.html#pathologist-team-annotations",
    "title": "Step 4: Patching",
    "section": "1. Pathologist Team Annotations",
    "text": "1. Pathologist Team Annotations\nBefore this step, we will send matched and extracted epithelium slices to the pathologist team to annotate the regions."
  },
  {
    "objectID": "step4.html#orientation",
    "href": "step4.html#orientation",
    "title": "Step 4: Patching",
    "section": "2. Orientation",
    "text": "2. Orientation\nAfter this we will orient the images to be aligned evenly before patching.\n\n\n\n\n\nOnce these steps are complete, we can begin patching."
  },
  {
    "objectID": "step4.html#description-of-code",
    "href": "step4.html#description-of-code",
    "title": "Step 4: Patching",
    "section": "Description of code",
    "text": "Description of code\nDirectories:\nDefines input_folder (where processed images are stored) and output_folder (where the results will be saved).\nDirectory structure:\nImages: Step 4/ Processed_images_sub\nFiltered_images: epithelium_patches_6_hori.py\n\n\n\n\n\n\nStep-by-Step Manual\n\n\n\n\n\n\nPlace all desired sample images in “filtered_images” folder\nOpen and run epithelium_patches_6_hori.py. Dependencies include cv2, numpy, os\nFind processed images in processed_images_sub\n\n\nResults\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\n\n\ncalculate_optimal_patch_dimensions(mask): Analyzes the epithelium mask to determine the optimal patch size for image processing. Finds the maximum continuous width of white pixels (epithelium) in each row to determine the optimal patch height. Calculates the patch width based on the total epithelium area, ensuring it’s large enough to cover meaningful portions of the image.\ncalculate_overlap(patch_coords, placed_patches): Checks if a new patch overlaps with any already placed patches. Returns True if the overlap area exceeds 10% of the patch’s total area, indicating that the patch should not be placed.\ncalculate_coverage(mask, patches): Calculates the coverage percentage of epithelium in the provided mask by summing the areas covered by all patches.Returns the percentage of epithelium covered by patches.\napply_patches(epithelium_mask, patch_height, patch_width, stride, orientation): Applies patches to the epithelium mask, either vertically or horizontally. Iterates over the image using a sliding window approach, placing patches where the epithelium ratio is 50% or more. Checks for overlaps using calculate_overlap and ensures patches are placed in non-overlapping areas.\nprocess_image(image_path, output_image_path, output_mask_path, region_outline_path): Reads and Preprocesses Image: Loads the image, converts it to grayscale, applies Gaussian blur, and thresholds to create a binary mask.\nMask Cleaning: Applies morphological operations (close and open) to clean the mask.\nEpithelium Detection: Finds contours, identifies the largest contour, and creates an epithelium mask.\nPatch Calculation: Uses calculate_optimal_patch_dimensions to determine the optimal patch size and stride for patch placement.\nApply Patches: Uses apply_patches for both vertical and horizontal patch orientations and calculates the coverage for each.\nRegion Outline: Draws rectangles around regions in the original image and saves an image showing the regions where patches will be applied.\nDraw Patches: Based on the coverage, selects the best orientation (vertical or horizontal) and draws patches on the image.\nSave Output: Saves the final processed image with patches drawn, as well as the region outline image and the epithelium mask.\nImage Processing Loop: Loops through all image files in the input_folder with .tif, .jpg, or .png extensions. For each image, it processes the image and saves: the epithelium mask, the final image with mixed patches applied, the region outline image showing the patches’ locations.\nPrints a message indicating that the processing is complete for all images.\nHardcoding: Only hardcoded value relates to the color thresholding used in the epithelium extraction. Other group members / team 6 have been working on ways to generalize this segment and can likely be combined."
  },
  {
    "objectID": "step4.html#results",
    "href": "step4.html#results",
    "title": "Step 4: Patching",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "step3.html",
    "href": "step3.html",
    "title": "Step 3: Epithelium Detection",
    "section": "",
    "text": "Aim: To separate the epithelium from the stroma in each tissue."
  },
  {
    "objectID": "step3.html#api-for-epithelium-extraction",
    "href": "step3.html#api-for-epithelium-extraction",
    "title": "Step 3: Epithelium Detection",
    "section": "API for Epithelium Extraction",
    "text": "API for Epithelium Extraction"
  },
  {
    "objectID": "step2.html",
    "href": "step2.html",
    "title": "Step 2: Matching slices across stains",
    "section": "",
    "text": "Aim: To identify and match structurally similar slices across stains for each patient."
  },
  {
    "objectID": "step2.html#results",
    "href": "step2.html#results",
    "title": "Step 2: Matching slices across stains",
    "section": "Results",
    "text": "Results\nExample:\nAll tissue slices from patient h2114153:\n      \nSuccessfully matched results"
  }
]