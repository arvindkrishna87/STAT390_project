[
  {
    "objectID": "step5.html",
    "href": "step5.html",
    "title": "Step 5: TBD",
    "section": "",
    "text": "Aim: We will find this out soon!\nMethodology:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2024 STAT390 (CMIL Classification)",
    "section": "",
    "text": "Aim: The objective of the project is to develop a machine learning model to accurately classify Conjunctival melanocytic intraepithelial lesions (C-MIL) as per the WHO 2022 classification system. In layman’s terms, we want to identify the severity of potential eye cancer by looking at a particular eye tissue of the patient.\nThe motivation behind the project is to provide a reproducible and accurate grading of C-MIL so that the most appropriate management plan for the patient can be developed. By improving the precision of lesion classification, our model has the potential to enhance patient outcomes and optimize treatment strategies\nThe Northwestern University STAT390 Class of Fall 2024 has made the following progress on this project:\n\nExtracting each tissue from the patient data using QuPath\nMatching tissue slices across the different stains (H&E, Melan-A, Sox-10) to each patient if there is a correct match\nDetecting and extracting only the epithelium for each tissue\nPatching certain regions of each epithelium for further examination\nTBD"
  },
  {
    "objectID": "step1.html",
    "href": "step1.html",
    "title": "Step 1: Slice extraction",
    "section": "",
    "text": "Aim: To extract the tissue slices from the WSI (Whole slide images)\nMethodology: QuPath scripting files to automate tissue extraction"
  },
  {
    "objectID": "step1.html#setting-up-in-qupath",
    "href": "step1.html#setting-up-in-qupath",
    "title": "Step 1: Slice extraction",
    "section": "Setting up in QuPath",
    "text": "Setting up in QuPath\nImages in a QuPath Project –&gt; processed_data/tissue_image.tif Contains tissues_1.json, tissues_2.json and automate_export_newest.groovy New for other stain types to expedite process: existing_annotations.groovy\n\nOpen a project that has all your images\nPut tissue_2.json into base_proj_dir/classifiers/pixel_classifiers (make if does not exist) Note, use tissues_2.json for most recent results (not tissues_1 but you can still try this too. tissues_2 contains broader parameters for a more sensitive model, works on more stains and images)\nPut automate_export_newest.groovy into base_project_dir/scripts (make if does not exist)\nMake sure you have an image open in QuPath interface\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; automate_export_newest.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir"
  },
  {
    "objectID": "step1.html#to-deal-with-more-difficult-stain-types-if-you-decide-to-manually-annotate",
    "href": "step1.html#to-deal-with-more-difficult-stain-types-if-you-decide-to-manually-annotate",
    "title": "Step 1: Slice extraction",
    "section": "To deal with more difficult stain types if you decide to manually annotate:",
    "text": "To deal with more difficult stain types if you decide to manually annotate:\n\nRuns like automate_export_newest.groovy but only if you already have annotations\n\nNeed to set annotation class to “Positive” in QuPath (Annotations –&gt; Positive –&gt; Set selected and for future annotations to be auto “Positive,” press “Auto set”“)\nTo export existing annotations only, run existing_annotations.groovy\nexisting_annotations.groovy –&gt; base_project_dir/scripts\nIn QuPath, top bar –&gt; Automate –&gt; Project scripts –&gt; existing_annotations.groovy\nScript Editor has three vertical dots at the bottom –&gt; Run for project\nData will save in processed_data dir in your base project dir"
  },
  {
    "objectID": "step1.html#to-create-a-new-pixel-classifier-or-modify-optional",
    "href": "step1.html#to-create-a-new-pixel-classifier-or-modify-optional",
    "title": "Step 1: Slice extraction",
    "section": "To create a new pixel classifier or modify (optional):",
    "text": "To create a new pixel classifier or modify (optional):\n\nQuPath Interface top bar –&gt; Classify –&gt; Pixel Classification –&gt; Create thresholder\nSee tissues_1.json and tissues_2.json for my parameters, and you can work from there\nSave this and then replace “tissues_2” in .groovy script."
  },
  {
    "objectID": "step1.html#step-1-first-pass-of-algorithm",
    "href": "step1.html#step-1-first-pass-of-algorithm",
    "title": "Step 1: Slice extraction",
    "section": "Step 1: First pass of algorithm",
    "text": "Step 1: First pass of algorithm\nFollowing the instructions above, open your image in QuPath and run this “annotation export newest” groovy script.\n\nSelect Run, then Run For Project\n\nNote: If your automation fails while running due to a particularly large image or systematically fails on a stain type (ie Sheffield Sox10–most fail because reference image annotation is too large to export), you have two options: Manually annotate and export images (more on this later) Downsample an annotated area (last resort, but can successfully downsample up to a factor of 2 to match stakeholder’s desired resolution), can do this directly by changing the downsample parameter\nSelect your images to process. Not counting the mask images, I tended to process up to 20 at a time to reduce the memory load."
  },
  {
    "objectID": "step1.html#step-2-analyze-results-and-troubleshoot",
    "href": "step1.html#step-2-analyze-results-and-troubleshoot",
    "title": "Step 1: Slice extraction",
    "section": "Step 2: Analyze results and troubleshoot",
    "text": "Step 2: Analyze results and troubleshoot\nOnce you run the automation for your images, I check in QuPath directly image by image to ensure all data was properly exported. You should also check in the processed_images dir created in your Qupath project dir that no image was corrupted or too blurry. In order of manual work needed, here are the possible cases for your images. They correspond with how we dealt with and logged processing these images in the Tracker Data of Status of Each Slice"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for the Fall 2024 STAT390 project!\nProfessor and Project Leader: Arvind Krishna\nTeam Members:\nMarianne Cano, Lauren Gomez, Rayyana Hassan, Baron Patterson, Cara Chang, Eli Nacar, Kenny Yeon, Luna Nguyen, Teresa Pan, Emma Drisko, Ally Bardas, Diego Goldfrank, Andrew Luke, Maddy Gallagher, Sarah Abdulwahid, Noah Schulhof, Christina Tzavara, Faith Cramer, Aryaman Chawla, Parth Patel, Atharva Weling, Kyla Bruno, Kevin Li, Alyssa Shou, Annie Xia, Alex Devorsetz, Kelly Meng\nStakeholders: Dr. Yamini Krishna, Dr. He Zhao, Prashant Kumar\nLink to connect to our Github link to connect to our API"
  },
  {
    "objectID": "about.html#data-overview",
    "href": "about.html#data-overview",
    "title": "About",
    "section": "Data overview",
    "text": "Data overview\nData Source\nData consists of 105 cases from 97 patients. Below are some statistics of the data:\n\n60 women (age range: 23-93 years; median 65; mean 61.5) and 37 men (age range: 31-91 years; median 68; mean 66.5)\nThe ethnic group mix comprised of the following: 88 White Caucasian, 4 Black, 2 South Asian, 1 Inuit, 1 Mixed race, and 1 Unspecified\n\nThe data corresponds to patients in the following 3 ocular oncology/pathology centers:\n\nLiverpool University Hospitals NHS Foundation Trust (Liverpool; cases from 2018 to 2021),\nRoyal Hallamshire Hospital (Sheffield; from 2011 to 2021), and\nRigshospitalet (Copenhagen; from 1996 to 2021)\n\nFor each patient, a tissue is taken from their conjunctiva. The tissue is sliced into multiple slices. Multiple slices are used so that each slice can be analyzed separately and there is more evidence to support the conclusion, i.e., the classification of the C-MIL."
  },
  {
    "objectID": "step4.html",
    "href": "step4.html",
    "title": "Step 4: Patching",
    "section": "",
    "text": "Aim: Applies horizontal / vertical patching across the epithelium for later use in model.\nMethodology: Patching algorithm calculates the optimal patch height and width based on the epithelium mask. The patch dimensions are based on the maximum continuous width of white pixels (i.e., the epithelium area) and the overall epithelium area in the mask. The patches are used to analyze and visualize the epithelium coverage, and they are placed on the images based on optimal patch dimensions and overlap calculations. Patching algorithm takes in processed images of the epithelium from the stained cells and applies a sliding kernel across the image The optimal patch height is the maximum width of continuous white pixels (in each row), with a minimum height of 100. The patch width is calculated by dividing the epithelium area by 100 and the optimal patch height."
  },
  {
    "objectID": "step4.html#description-of-code",
    "href": "step4.html#description-of-code",
    "title": "Step 4: Patching",
    "section": "Description of code",
    "text": "Description of code\nDirectories: Defines input_folder (where processed images are stored) and output_folder (where the results will be saved).\nDirectory structure: Step 4/ Processed_images_sub Filtered_images epithelium_patches_6_hori.py"
  },
  {
    "objectID": "step4.html#set-up",
    "href": "step4.html#set-up",
    "title": "Step 4: Patching",
    "section": "Set Up",
    "text": "Set Up\nPlace all desired sample images in “filtered_images” folder\nOpen and run epithelium_patches_6_hori.py. Dependencies include cv2, numpy, os\nFind processed images in “processed_images_sub”"
  },
  {
    "objectID": "step4.html#functions-explanations",
    "href": "step4.html#functions-explanations",
    "title": "Step 4: Patching",
    "section": "Functions Explanations:",
    "text": "Functions Explanations:\ncalculate_optimal_patch_dimensions(mask): Analyzes the epithelium mask to determine the optimal patch size for image processing. Finds the maximum continuous width of white pixels (epithelium) in each row to determine the optimal patch height. Calculates the patch width based on the total epithelium area, ensuring it’s large enough to cover meaningful portions of the image.\ncalculate_overlap(patch_coords, placed_patches): Checks if a new patch overlaps with any already placed patches. Returns True if the overlap area exceeds 10% of the patch’s total area, indicating that the patch should not be placed.\ncalculate_coverage(mask, patches): Calculates the coverage percentage of epithelium in the provided mask by summing the areas covered by all patches.Returns the percentage of epithelium covered by patches.\napply_patches(epithelium_mask, patch_height, patch_width, stride, orientation): Applies patches to the epithelium mask, either vertically or horizontally. Iterates over the image using a sliding window approach, placing patches where the epithelium ratio is 50% or more. Checks for overlaps using calculate_overlap and ensures patches are placed in non-overlapping areas.\nprocess_image(image_path, output_image_path, output_mask_path, region_outline_path): Reads and Preprocesses Image: Loads the image, converts it to grayscale, applies Gaussian blur, and thresholds to create a binary mask.\nMask Cleaning: Applies morphological operations (close and open) to clean the mask.\nEpithelium Detection: Finds contours, identifies the largest contour, and creates an epithelium mask.\nPatch Calculation: Uses calculate_optimal_patch_dimensions to determine the optimal patch size and stride for patch placement.\nApply Patches: Uses apply_patches for both vertical and horizontal patch orientations and calculates the coverage for each.\nRegion Outline: Draws rectangles around regions in the original image and saves an image showing the regions where patches will be applied.\nDraw Patches: Based on the coverage, selects the best orientation (vertical or horizontal) and draws patches on the image.\nSave Output: Saves the final processed image with patches drawn, as well as the region outline image and the epithelium mask.\nImage Processing Loop: Loops through all image files in the input_folder with .tif, .jpg, or .png extensions. For each image, it processes the image and saves: the epithelium mask, the final image with mixed patches applied, the region outline image showing the patches’ locations.\nPrints a message indicating that the processing is complete for all images.\nHardcoding: Only hardcoded value relates to the color thresholding used in the epithelium extraction. Other group members / team 6 have been working on ways to generalize this segment and can likely be combined."
  },
  {
    "objectID": "step4.html#results",
    "href": "step4.html#results",
    "title": "Step 4: Patching",
    "section": "Results",
    "text": "Results\nImages for Alyssa’s code"
  },
  {
    "objectID": "step3.html",
    "href": "step3.html",
    "title": "Step 3: Epithelium Detection",
    "section": "",
    "text": "Aim: To detect and extract the epithelium in each slice.\nMethodology:"
  },
  {
    "objectID": "step3.html#set-up",
    "href": "step3.html#set-up",
    "title": "Step 3: Epithelium Detection",
    "section": "Set Up",
    "text": "Set Up"
  },
  {
    "objectID": "step3.html#results",
    "href": "step3.html#results",
    "title": "Step 3: Epithelium Detection",
    "section": "Results",
    "text": "Results\nImages for Kevin’s updated Prashant code."
  },
  {
    "objectID": "step2.html",
    "href": "step2.html",
    "title": "Step 2: Matching slices across stains",
    "section": "",
    "text": "Aim: To identify structurally similar slices across stains.\nMethodology:\nTwo codes required:\nMatching Algorithm Evaluation Tracker"
  },
  {
    "objectID": "step2.html#step-by-step-manual",
    "href": "step2.html#step-by-step-manual",
    "title": "Step 2: Matching slices across stains",
    "section": "Step-by-Step Manual",
    "text": "Step-by-Step Manual\nFollow these steps to use the pipeline to generate matching slices for all patients or a specific subset:\nPrepare Image Data: Place .tif image files in the designated folder (e.g., processed_images). Ensure that each patient has images for all three stains (H&E, Melanin, Sox10).\nChoose Full or Subset Processing:\nFor All Patients: Place all images in the same processed_images directory. For a Subset of Patients: Separate patient image files into their own folders within processed_images (e.g., processed_images/patient1, processed_images/patient2). Run the Pipeline:\nRun match_pipeline.py. A GUI will prompt you to select the directory containing the images. Open NoahsMatching.ipynb in Jupyter Notebook and execute cells in sequence. Process and Review Matches: Upon execution, the pipeline will preprocess, extract contours, and match images, saving results in the matches folder. Images will be organized by patient, with matched slices grouped in subfolders."
  },
  {
    "objectID": "step2.html#results",
    "href": "step2.html#results",
    "title": "Step 2: Matching slices across stains",
    "section": "Results",
    "text": "Results\nFor 1 patient, show all the slices. then Show images of the matching slices it chose"
  }
]